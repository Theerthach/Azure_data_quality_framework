{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e7e973-5ace-4a16-b00b-b0d340c66376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver=spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .option(\"inferSchema\",\"true\")\\\n",
    "            .load('abfss://bronze@dqcheckstheertha.dfs.core.windows.net/part-00000-ccfc6224-03f4-4835-a75c-d76a0ed69f9a-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7cbe381-bd0c-43d9-97cd-aabbf6083135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- OrderID: integer (nullable = true)\n |-- CustomerKey: string (nullable = true)\n |-- OrderDate: date (nullable = true)\n |-- ProductCode: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: string (nullable = true)\n |-- ShippingRegion: string (nullable = true)\n |-- IsProcessed: boolean (nullable = true)\n |-- IsErrorRow: boolean (nullable = true)\n |-- IsInvalidCustomer: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48373214-4c25-4e4e-b87b-73a5da572323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCCA *** DATA QUALITY RESULTS ***\n{\n  \"row_count\": 12,\n  \"OrderID_nulls\": 0,\n  \"CustomerKey_nulls\": 1,\n  \"OrderDate_nulls\": 0,\n  \"ProductCode_nulls\": 0,\n  \"Quantity_nulls\": 0,\n  \"UnitPrice_nulls\": 0,\n  \"Quantity_invalid_values\": 1,\n  \"UnitPrice_invalid_values\": 2,\n  \"OrderDate_bad\": 0,\n  \"duplicate_OrderID\": 0,\n  \"IsErrorRow_true\": 4,\n  \"IsInvalidCustomer_true\": 2\n}\n\n❌ *** DATA QUALITY FAILURES ***\n- ❌ CustomerKey contains 1 null/blank values.\n- ❌ Quantity has 1 invalid or non-positive values.\n- ❌ UnitPrice has 2 invalid or non-positive values.\n- ⚠️ 4 rows flagged as IsErrorRow=True.\n- ⚠️ 2 rows flagged as invalid customers.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5699680987073200>, line 147\u001B[0m\n",
       "\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m failures:\n",
       "\u001B[1;32m    146\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m, f)\n",
       "\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Data Quality Checks Failed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m✔ All Data Quality Checks Passed Successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: ❌ Data Quality Checks Failed."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Exception",
        "evalue": "❌ Data Quality Checks Failed."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Exception</span>: ❌ Data Quality Checks Failed."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5699680987073200>, line 147\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m failures:\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m, f)\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Data Quality Checks Failed.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m✔ All Data Quality Checks Passed Successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mException\u001B[0m: ❌ Data Quality Checks Failed."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "#        DATA QUALITY FRAMEWORK FOR df_silver\n",
    "# ==========================================================\n",
    "\n",
    "from pyspark.sql.functions import col, isnan, to_date, countDistinct\n",
    "from pyspark.sql.types import IntegerType, DoubleType, FloatType, DateType, LongType, DecimalType\n",
    "\n",
    "# Ensure df_silver exists\n",
    "try:\n",
    "    df = df_silver\n",
    "except NameError:\n",
    "    raise Exception(\" df_silver not found. Ensure pipeline loads df_silver before this notebook.\")\n",
    "\n",
    "results = {}\n",
    "failures = []\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 1️⃣ BASIC CHECKS\n",
    "# ==========================================================\n",
    "\n",
    "row_count = df.count()\n",
    "results[\"row_count\"] = row_count\n",
    "\n",
    "if row_count == 0:\n",
    "    failures.append(\" df_silver is empty.\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2️⃣ NULL CHECKS by column type\n",
    "# ==========================================================\n",
    "\n",
    "required_cols = [\n",
    "    \"OrderID\",\n",
    "    \"CustomerKey\",\n",
    "    \"OrderDate\",\n",
    "    \"ProductCode\",\n",
    "    \"Quantity\",\n",
    "    \"UnitPrice\"\n",
    "]\n",
    "\n",
    "df_dtypes = dict(df.dtypes)\n",
    "\n",
    "for c in required_cols:\n",
    "\n",
    "    dtype = df_dtypes[c].lower()\n",
    "\n",
    "    # Numeric columns → allow isnan()\n",
    "    if dtype in [\"int\", \"integer\", \"bigint\", \"double\", \"float\", \"long\", \"decimal\"]:\n",
    "        null_count = df.filter(col(c).isNull() | isnan(c)).count()\n",
    "\n",
    "    # Non-numeric → string/date/boolean\n",
    "    else:\n",
    "        null_count = df.filter(col(c).isNull() | (col(c) == \"\")).count()\n",
    "\n",
    "    results[f\"{c}_nulls\"] = null_count\n",
    "\n",
    "    if null_count > 0:\n",
    "        failures.append(f\" {c} contains {null_count} null/blank values.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3️⃣ DATA TYPE / VALUE VALIDATION\n",
    "# ==========================================================\n",
    "\n",
    "# --- Numeric must be > 0 ---\n",
    "numeric_positive_cols = [\"Quantity\", \"UnitPrice\"]\n",
    "\n",
    "for c in numeric_positive_cols:\n",
    "    invalid = df.filter(\n",
    "        (col(c).cast(\"double\").isNull()) | \n",
    "        (col(c).cast(\"double\") <= 0)\n",
    "    ).count()\n",
    "    \n",
    "    results[f\"{c}_invalid_values\"] = invalid\n",
    "\n",
    "    if invalid > 0:\n",
    "        failures.append(f\" {c} has {invalid} invalid or non-positive values.\")\n",
    "\n",
    "\n",
    "# --- Date validation ---\n",
    "if \"OrderDate\" in df.columns:\n",
    "    bad_dates = df.filter(to_date(col(\"OrderDate\")).isNull()).count()\n",
    "\n",
    "    results[\"OrderDate_bad\"] = bad_dates\n",
    "\n",
    "    if bad_dates > 0:\n",
    "        failures.append(f\" OrderDate contains {bad_dates} invalid date formats.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4️⃣ DUPLICATE CHECKS\n",
    "# ==========================================================\n",
    "\n",
    "# Choose which columns define uniqueness\n",
    "unique_key = [\"OrderID\"]\n",
    "\n",
    "dup_count = (\n",
    "    df.groupBy(unique_key)\n",
    "      .count()\n",
    "      .filter(col(\"count\") > 1)\n",
    "      .count()\n",
    ")\n",
    "\n",
    "results[\"duplicate_OrderID\"] = dup_count\n",
    "\n",
    "if dup_count > 0:\n",
    "    failures.append(f\" Found {dup_count} duplicate OrderID records.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5️⃣ BUSINESS RULE CHECKS\n",
    "# ==========================================================\n",
    "\n",
    "# Rule: If IsErrorRow = true, row should not be processed\n",
    "if \"IsErrorRow\" in df.columns:\n",
    "    error_rows = df.filter(col(\"IsErrorRow\") == True).count()\n",
    "    results[\"IsErrorRow_true\"] = error_rows\n",
    "    if error_rows > 0:\n",
    "        failures.append(f\" {error_rows} rows flagged as IsErrorRow=True.\")\n",
    "\n",
    "# Rule: CustomerKey should not be invalid\n",
    "if \"IsInvalidCustomer\" in df.columns:\n",
    "    invalid_cust = df.filter(col(\"IsInvalidCustomer\") == True).count()\n",
    "    results[\"IsInvalidCustomer_true\"] = invalid_cust\n",
    "    if invalid_cust > 0:\n",
    "        failures.append(f\"{invalid_cust} rows flagged as invalid customers.\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6️⃣ FINAL OUTPUT\n",
    "# ==========================================================\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA *** DATA QUALITY RESULTS ***\")\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "if failures:\n",
    "    print(\"\\n *** DATA QUALITY FAILURES ***\")\n",
    "    for f in failures:\n",
    "        print(\"-\", f)\n",
    "    raise Exception(\" Data Quality Checks Failed.\")\n",
    "else:\n",
    "    print(\"\\n✔ All Data Quality Checks Passed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb9a079-476d-4938-9530-02946f5b2b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "dq_output = []\n",
    "\n",
    "for k, v in results.items():\n",
    "    dq_output.append(Row(\n",
    "        check_name=k,\n",
    "        check_value=v,\n",
    "        pipeline_run_time=datetime.now().isoformat()\n",
    "    ))\n",
    "\n",
    "dq_df = spark.createDataFrame(dq_output)\n",
    "\n",
    "dq_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"dq_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354e79fa-9b83-4647-8d30-41d21da080cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dq_df.write.format(\"delta\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .save(\"abfss://silver@dqcheckstheertha.dfs.core.windows.net/dq_results\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6185351463070187,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DQ_Checks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}